{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a corpus of BNC documents for topic modelling \n",
    "\n",
    "The following code will create a corpus, which is the large subset of the entire BNC, that can be used with bag of words topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from bnctools import utils\n",
    "\n",
    "utils.vocabulary_directory = 'cache'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a quick and dirty checksum-er to check file integrity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checksum(filename):\n",
    "\n",
    "    '''\n",
    "    Returns the hash checksum of the file named `filename`.\n",
    "    '''\n",
    "\n",
    "    h = hashlib.new('sha256')\n",
    "\n",
    "    argument = open(filename,'rb').read()\n",
    "\n",
    "    h.update(argument)\n",
    "\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse BNC into paragraphs\n",
    "\n",
    "Extract all paragraphs from the BNC using some tools in the `bnctools.utils` module. These paragraphs are available in the cache (available via `git fat pull`) or can be calculated a new using the rather computationally expensive `get_all_paragraphs_parallel` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cached_data = True # Set to False if you do not want to use the cached pickle file\n",
    "\n",
    "pkl_filename = 'cache/bnc_paragraphs.pkl'\n",
    "pkl_file_checksum = '0b70c19a5ef8243933368d93ec47a4bf35674c04808e198d249a0e20e575f7b4'\n",
    "\n",
    "if not use_cached_data:\n",
    "\n",
    "    bnc_2554_texts_root = 'cache/bnc/2554/download/Texts/'\n",
    "\n",
    "    corpus_filenames = utils.Corpus.get_corpus_filenames(bnc_2554_texts_root)\n",
    "    \n",
    "    # Make sure cluster is started with e.g. ipcluster start -n 16\n",
    "    view = utils.init_ipyparallel()\n",
    "    paragraphs = utils.get_all_paragraphs_parallel(view, corpus_filenames)\n",
    "    utils.dump(paragraphs, filename=pkl_filename)\n",
    "    \n",
    "    assert checksum('cache/bnc_paragraphs.pkl') == pkl_file_checksum\n",
    "\n",
    "else:\n",
    "    \n",
    "    assert checksum('cache/bnc_paragraphs.pkl') == pkl_file_checksum\n",
    "\n",
    "    paragraphs = utils.load(pkl_filename)\n",
    "\n",
    "assert sum(map(lambda paragraph: paragraph['word_count'], paragraphs)) == 87564696"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove selected paragraphs \n",
    "\n",
    "Those paragraphs that were used as experimental stimuli in experiments *Brisbane* and *Malmo* should be removed from the corpus. The intended use of the corpus is to be the training set for the Topic model that by hypothesis represents the average background knowledge of participants who then read and memorize texts or word lists in experiments. Including those texts, or the texts from which the word lists were derived, in the training corpus would not be appropriate  because it would in a sense be putting those texts in the participants' background knowledge.\n",
    "\n",
    "The paragraphs used in the above mentioned experiments are available in `sampled-stimuli.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stimuli_filename = 'cache/sampled-stimuli.pkl'\n",
    "\n",
    "assert checksum(stimuli_filename)\\\n",
    "    == 'fbc45d8f87479bd7290c4d1e848b310294c28c5120dc5d42848a07214d26d774'\n",
    "\n",
    "with open(stimuli_filename, 'rb') as f:\n",
    "    experimental_stimuli = utils.pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find the ID, defined by BNC filename, div1 index in file, paragraph index, of the experimental stimuli paragraphs and then filter them out of the entire paragraph list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_paragraph_id(paragraph):\n",
    "    return tuple(\n",
    "        map(paragraph.get, \n",
    "            ('corpus_filename', 'div1_index', 'paragraph_index')\n",
    "           )\n",
    "    )\n",
    "\n",
    "experimental_stimuli_ids = map(get_paragraph_id, experimental_stimuli)\n",
    "\n",
    "# Hack; We need to strip off the ./ from the stimuli filenames\n",
    "_tmp = []\n",
    "for experimental_stimuli_id in experimental_stimuli_ids:\n",
    "    filename, div1_index, paragraph_index = experimental_stimuli_id\n",
    "    _tmp.append((filename.strip('./'), div1_index, paragraph_index))\n",
    "experimental_stimuli_ids = _tmp\n",
    "del _tmp\n",
    "\n",
    "acceptable_paragraphs\\\n",
    "    = filter(lambda paragraph: not get_paragraph_id(paragraph) in experimental_stimuli_ids, \n",
    "             paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will create a set of small \"documents\". Each document is either a single paragraph or a concatenation of consecutive paragraphs such that the total word count in each mini document is in a given word count range, which by default is 250 to 500 words. Each mini-doc is represented as a string with words delimited by a '|'. We'll write the corpus of documents, and the vocabulary, to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_corpus_to_file(mini_documents, \n",
    "                         corpus_filename_check,\n",
    "                         vocab_filename_check,\n",
    "                         corpus_checksum, \n",
    "                         vocab_checksum,\n",
    "                         cache_directory='cache'):\n",
    "    \n",
    "    counts = map(lambda doc: len(doc.split('|')), mini_documents)\n",
    "\n",
    "    corpus_filename = 'bnc_texts_%d_%d_%d_%d.txt' % tuple([func(counts) for func in (sum, len, min, max)])\n",
    "    corpus_file_path = os.path.join(cache_directory, corpus_filename)\n",
    "\n",
    "    with open(corpus_file_path, 'w') as f:\n",
    "        f.write('\\n'.join(mini_documents))\n",
    "\n",
    "    assert corpus_filename == corpus_filename_check, corpus_filename\n",
    "    assert checksum(corpus_file_path) == corpus_checksum\n",
    "\n",
    "    vocabulary = utils.get_corpus_vocabulary(mini_documents, minimum_count=5)\n",
    "    vocab_filename = 'bnc_vocab_%d.txt' % len(vocabulary)\n",
    "    vocab_file_path = os.path.join(cache_directory, vocab_filename)\n",
    "\n",
    "    with open(vocab_file_path, 'w') as f:\n",
    "        f.write('\\n'.join(sorted(vocabulary.keys())))\n",
    "\n",
    "    assert vocab_filename == vocab_filename_check, vocab_filename\n",
    "    assert checksum(vocab_file_path) == vocab_checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_documents = utils.paragraphs_to_mini_documents(acceptable_paragraphs,\n",
    "                                                    mini_document_length=(250, 500),\n",
    "                                                    sep='|')\n",
    "\n",
    "write_corpus_to_file(mini_documents,\n",
    "                     'bnc_texts_78639361_183975_251_499.txt',\n",
    "                     'bnc_vocab_49324.txt',\n",
    "                     'bd91a2936157f50b0ceb3cf9430a53d4c652e3ab553b8ec14fe75db6e07cd36c',\n",
    "                     'ecf66c77121cf67e416580cf5cc0853bd1813dcfd946298723134e547324cb6b')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
